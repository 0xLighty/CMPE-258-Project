# -*- coding: utf-8 -*-
"""ImageCaptioning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L2XTGVa64u7ibuXVNysRX1t0SbPv21SZ

### Import Modules
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import efficientnet
from tensorflow.keras.layers import TextVectorization
import matplotlib.pyplot as plt

import collections
import random
import numpy as np
import os
import time
import json
from PIL import Image

"""### Download and Load the MS-COCO Dataset

LOAD DATASET
"""

# Download caption annotation files
annotation_folder = '/annotations/'
if not os.path.exists(os.path.abspath('.') + annotation_folder):
  annotation_zip = tf.keras.utils.get_file('captions.zip',
                                           cache_subdir=os.path.abspath('.'),
                                           origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',
                                           extract=True)
  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'
  os.remove(annotation_zip)

"""#### Imagedata"""

image_folder = '/train2014/'
image_zip = tf.keras.utils.get_file('train2014.zip',
                                      cache_subdir=os.path.abspath('.'),
                                      origin='http://images.cocodataset.org/zips/train2014.zip',
                                      extract=True)
PATH = os.path.dirname(image_zip) + image_folder
os.remove(image_zip)
print(PATH)

"""#### Captions Data"""

annotation_folder = '/annotations/'
annotation_zip = tf.keras.utils.get_file('captions.zip',
                                          cache_subdir=os.path.abspath('.'),
                                          origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',
                                          extract=True)
annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'
os.remove(annotation_zip)

"""#### Limiting the Dataset Size"""

# Group all the captions with the same image id and take only the first 6000 Images from the grouped dataset.
def group_image_captions_and_reduce():
  with open(annotation_file, 'r') as f:
    annotations = json.load(f)
  image_path_to_caption = collections.defaultdict(list)
  for val in annotations['annotations']:
    caption = f"<start> {val['caption']} <end>"
    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])
    image_path_to_caption[image_path].append(caption)
  image_paths = list(image_path_to_caption.keys())
  random.shuffle(image_paths)
  train_image_paths = image_paths[:6000]
  print(len(train_image_paths))
  train_captions = []
  img_name_vector = []
  for image_path in train_image_paths:
    caption_list = image_path_to_caption[image_path]
    train_captions.extend(caption_list)
    img_name_vector.extend([image_path] * len(caption_list))
  return train_captions, img_name_vector

train_capptions, img_name_vector = group_image_captions_and_reduce()



"""### Preprocess the Images"""

# We are preprocessing our images using a pretrained InceptionV3 model from Keras with ImageNet weights.
# For the preprocessing, we read the image data, resize it to 299px by 299px. 
# Using the preprocessor_input function, we normalize the image pixels to be in the -1 to 1 range
# which matches the format of the images used to train InceptionV3.

def load_image(image_path):
    img = tf.io.read_file(image_path)
    img = tf.io.decode_jpeg(img, channels=3)
    img = tf.keras.layers.Resizing(299, 299)(img)
    img = tf.keras.applications.inception_v3.preprocess_input(img)
    return img, image_path

image_model = tf.keras.applications.InceptionV3(include_top=False,
                                                weights='imagenet')
new_input = image_model.input
hidden_layer = image_model.layers[-1].output

image_features_extract_model = tf.keras.Model(new_input, hidden_layer)

"""#### Using tqdm to Cache Extracted Image Features"""

!pip install tqdm

# Use only unique image names
encode_train = sorted(set(img_name_vector))

image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)
image_dataset = image_dataset.map(
  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)

for img, path in tqdm(image_dataset):
  batch_features = image_features_extract_model(img)
  batch_features = tf.reshape(batch_features,
                              (batch_features.shape[0], -1, batch_features.shape[3]))

  for bf, p in zip(batch_features, path):
    path_of_feature = p.numpy().decode("utf-8")
    np.save(path_of_feature, bf.numpy())

"""### Preprocess and Tokenize the Captions"""

caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)

# Our captions are appended with <> tags(<start> <end>) as prefix and suffix 
# tags. Using default normalization of TextVectorization would remove these 
# tags. So we created our own standarize function to preserve these tags.
def standardize(inputs):
  inputs = tf.strings.lower(inputs)
  return tf.strings.regex_replace(inputs,
                                  r"!\"#$%&\(\)\*\+.,-/:;=?@\[\\\]^_`{|}~", "")

# We allow only a maximum of 50 words for each caption.
max_length = 50
# We create a vocabulary of 5000 words from these captions
vocabulary_size = 5000
tokenizer = tf.keras.layers.TextVectorization(
    max_tokens=vocabulary_size,
    standardize=standardize,
    output_sequence_length=max_length)
# Populate vocabulary from the caption data.
tokenizer.adapt(caption_dataset)

# Create the tokenized vectors
cap_vector = caption_dataset.map(lambda x: tokenizer(x))
# We created mappings for words to indices and indicies to words.
word_to_index = tf.keras.layers.StringLookup(
    mask_token="",
    vocabulary=tokenizer.get_vocabulary())
index_to_word = tf.keras.layers.StringLookup(
    mask_token="",
    vocabulary=tokenizer.get_vocabulary(),
    invert=True)

"""### Split the Data into Training and Testing Sets """

def split_data(ratio, image_names, captions):
  # creae default dictionary
  image_caption_dict = collections.defaultdict(list)

  # create image to caption map
  for image, caption in zip(image_names, captions):
    image_caption_dict[image].append(caption)

  # taking images(key) from above dictionary and shuffling it to have random shuffle
  keys = list(image_caption_dict.keys())
  random.shuffle(keys)

  split_index = int(len(keys) * ratio)
  
  # take names of images for training and validation
  train_image_name_keys, val_image_name_keys = keys[:split_index], keys[slice_index:]

  # creating training images and captions
  ret_train_image_name = list()
  ret_train_caption = list()
  for idx in train_image_name_keys:
    ret_train_image_name.extend([idx] * len(image_caption_dict[idx]))
    ret_train_caption.extend(image_caption_dict[idx])

  # creating validation images and captions
  ret_val_image_name = list()
  ret_val_caption = list()
  for idx in val_image_name_keys:
    ret_val_image_name.extend([idx] * len(image_caption_dict[idx]))
    ret_val_caption.extend(image_caption_dict[idx])

  return ret_train_image_name, ret_train_caption, ret_val_image_name, ret_val_caption

train_image_name, train_caption, val_image_name, val_caption = split_data(0.8, img_name_vector, cap_vector)

len(train_image_name), len(train_caption), len(val_image_name), len(val_caption)

train_image_name[:2]

train_caption[:2]

"""### Creating a dataset (image+captions)"""

BATCH_SIZE = 64
BUFFER_SIZE = 1000
embedding_dim = 256
units = 512
num_steps = len(train_image_name) // BATCH_SIZE
# Shape of the vector extracted from InceptionV3 is (64, 2048)
# These two variables represent that vector shape
features_shape = 2048
attention_features_shape = 64

# Load the numpy files
def map_func(img_name, cap):
  img_tensor = np.load(img_name.decode('utf-8')+'.npy')
  return img_tensor, cap

dataset = tf.data.Dataset.from_tensor_slices((train_image_name, train_caption))

# Use map to load the numpy files in parallel
dataset = dataset.map(lambda item1, item2: tf.numpy_function(
          map_func, [item1, item2], [tf.float32, tf.int64]),
          num_parallel_calls=tf.data.AUTOTUNE)

# Shuffle and batch
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)

"""### Model Creation and Training"""



"""### Caption Custom Images With the Model"""

